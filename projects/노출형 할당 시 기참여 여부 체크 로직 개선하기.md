
## Background

- 광고할당 시 광고들이 해당 유저가 이미 참여한 광고인지 확인 후 해당 광고 리워드에 0으로 할당하거나 필터링하고 있습니다.
    
- 기참여 광고인지 확인하는 방법은 point table에서 적립내역을 받아와서 기존 광고는 필터링합니다 .
    
- 헌데 기존 로직의 문제점으로 인해 유저가 interval 기간 내 재할당 및 적립을 받는 케이스가 발생하였습니다.
    

관련 코드 및 PR: (사내 정보)

## Problem

- [이미 참여한건지 bs point에서 point item들을 조회](https://github.com/Buzzvil/buzzscreen-api/blob/master/internal/services/usecases/reward/repo/dynamo.go#L9-L20)하면서 받아오게 되는데 이 수치가 limit 이 100으로 걸려있음.
    

SearchLimit과 Limit이 걸려있는데 SearchLimit 이 100으로 하드코딩 고정값임.

- 특정 기간 내 할당 및 적립이 되어선 안되는 기간이 존재하는데 해당 기간내 100번 넘게 이미 참여한 경우, 이 기간내 재할당이 될 수 있는 구조임.
    
- limit을 걸고 조회하는 방식은 limit을 높일 수록 overhead가 발생하고 limit 수치 이상으로 유저가 참여하지 않는다는 보장을 받을 수 없는 상황이다.
    

## How to Solve

현재 상수값으로 limit을 걸고 dynamodb point item들을 받아와 loop을 돌면서 하나씩 체크하는 것이 아닌 방식으로 구조를 개선한다.

Redis or 별도 Dynamodb table을 사용하여 deviceID + ad_id(lineitem_id) 조합 키로 재참여 방지 기간만큼 TTL을 설정한다.

## Considerations

- 현재 limit 100개를 들고 올 때 Dynamo에 IO가 몇번 발생할까요? 이를 Redis로 변경 시 키를 deviceID_AdID 와 같이 설정 시 각 광고마다 받아온 광고 수 만큼 IO를 발생시켜야 한다.  
    위와 같은 키 설정이 아닌 deviceID : [ createdAt_adID, createdAt_adID, createdAt_adID, … ]  
    sorted 로 넣고 가져오고 ttl 지난건 삭제하도록 사용하는 방법은 가능할지? 그럼 조회 시에 해당 시간이 지난건 삭제하도록하는 command를 같이 lua script를 짜거나 해서 한번에 묶어서 날려줘야할 것 같다. 이 방식은 single thread인 redis cpu 부하로 latency가 발생하진 않을지? 좀 더 괜찮은 자료구조나 redis를 더 잘 사용하는 방안이 있는지 찾아보는게 좋을 것 같음.
    
- 별도 dynamodb table을 사용 시 hash key(device_id , ad_id 이용해서) 값을 어떻게 설정해서 분산시킬지? 핫파티션키로부터 어떻게 안전하게 설정할건가?
    
- 해당 키를 언제 어떻게 insert 할지 고민방안
    
    - 1. 적립요청 kafka topic에 sink connector를 붙여서 키를 insert 한다.
        
        - 비동기 요청만 처리하고 있음으로 동기 로직은 별도 처리가 필요함. 현재는 bs_point에 쌓이는 것들은 동기로 적립되는 것이 없어서 괜찮지만 bs_point로 동기적립 케이스가 추후 생길경우 여기도 추가 작업을 해줘야하는 번거로움이 발생함.
            
    - 2. 적립완료 kafka topic에 sink connector를 붙여서 키를 insert 한다.
        
        - 적립 완료는 매체사로부터 적립 성공응답을 받아야 생성됩니다. 이 경우 매체사의 장애 혹은 문제로 인한 dependency가 발생합니다. 매체사에서 응답 지연이나 장애가 발생하는 동안 해당 지면에서 광고가 다시 재할당되는 문제가 우려됩니다.
            
    - 3. bs_point table에 insert될 때, 해당 키도 insert 한다.
        
        - BS 에서 해당 키를 insert하면 BS-API 에서 select하기에 각기 다른 서버에서 db를 공유하는 문제가 발생함. 두 서버가 추후 통합될 예정이라해도 bs 적립로직을 pointsvc 하나로 다 통합하고 싶은 방향을 고려하고 있는데 두개의 서버가 같은 db를 참조하게 됨.  
            이를 개선하기 위해서 insert 할 때 bs_point table에 source connector를 붙이고 카프카에 넣어서 sink connector로 조회할 db에 넣어주면 각기 다른 서버에서 같은 DB를 사용하는 문제를 해결할 순 있다.
            
    - 4. redis를 사용할 경우 bs_point와 각기 다른 두 db 혹은 dynamodb를 사용하면 별도 table에 insert가 되어야하는데 하나의 트랜잭션으로 보장이 필요한가?
        
        - 하나의 트랜잭션으로 묶어서 (적립용 bs_point insert + 광고 필터용 insert) 하나의 트랜잭션으로 관리하는게 오히려 적립 가용성을 떨어트리는 것 같음
            
        - 적립 bs_point item 생성 후 광고필터용 키를 insert시 현재 운영하는 방식과 달리 insert로직이 하나 더 추가되는데 여기서 실패하는 케이스가 발생할 수 있음. 이 경우 광고가 재할당되지만 적립 validation 로직이 돌면서 interval 내 재적립은 안되도록 되어있음. 광고를 봤음에도 리워드는 지급이 안되는 부분이라 CS가 강하게 들어오는 케이스가 발생함. 이 케이스를 개선할 방법은 없는 것인지? 이 측면에서 생각해보면, Redis보다 Dynamodb가 좀 더 고가용성 서비스인게 아닌가라는 관점에서 Dynamo에 한 표를 던져줄만함. 필터링 조회에서 fail이 발생하면 강성 CS가 발생하는데 버즈빌 근무하면서 Redis가 fail이 발생하는 케이스를 몇번 봤지만 Dynamo에서는 hot partition key 가능성만 없다면 fail 이 발생한 기억은 없음.
            
        - application에서 각 db에 insert io를 한번씩 보내는 것 보다 bs_point 에 source connector를 붙이고 sink connector를 이용해서 insert를 하면 application에서 io를 한번 더 발생시킬 필요가 없는 장점이 있다. application에서 직접 insert하는 것에 비해 실패 건도 적을 것 같다
            
- Redis vs Dynamodb 비용 계산해보기. redis 자료구조를 뭘 쓸지에 따라 비용이 달라질 가능 성이 있어서 redis 쓰면 어떻게 쓸지부터 찾아보기.
    

## Deployment

device id를 //100 연산으로 퍼센트 단위로 일정부분씩 트래픽을 올리면서 배포

## Due Date

1월 말에 라이브 예정되어 있음.

개인적인 의견으로는 우선 limit을 120, 150 수준으로 임시로 올려주고 대응이 가능한 상황으로 보임. (cs 들어오는 건들 tansaction_id 간에 차이를 보면 100초반에 형성되어 있음)